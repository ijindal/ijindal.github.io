---
title: 'Keeping LLMs Aligned Without the Cost: The Story of Instruction Residuals'
date: 2025-10-10
permalink: /posts/2012/08/blog-post-4/
tags:
  - cool posts
  - category1
  - category2
---

Keeping LLMs Aligned Without the Cost: The Story of Instruction Residuals

In the ever-evolving world of Large Language Models (LLMs), continual pretraining is becoming essential. We want our models to keep learning from new data. But thereâ€™s a catchâ€”every time a model learns something new, it tends to forget how to follow instructions. And retraining it to regain this ability? Thatâ€™s massively expensive.

What if we could skip that retraining altogether?

Thatâ€™s the central idea behind my recent work, â€œKeep the Alignment, Skip the Overhead: Lightweight Instruction Alignment for Continually Trained LLMsâ€. This research introduces a simple yet powerful mechanismâ€”Instruction Residualsâ€”to retain and restore instruction-following behavior in LLMs without full instruction fine-tuning.

I'm thrilled to share that this work has been accepted at the ICML 2025 Workshop on Test-Time Adaptation: Putting Updates to the Test!

ğŸ§  The Core Idea: Instruction Residuals

Instead of re-aligning the entire model after every update, we extract the instruction-following "difference" between a base LLM and its instruction-tuned version. This instruction residual acts like a plug-and-play adapter.

So, when you update your base model with new knowledge through continual pretraining, you can simply add back this residual to recover instruction capabilitiesâ€”no need to redo the whole instruction tuning process.

This not only saves compute (2000Ã— less!) but also preserves model architecture and makes adaptation modular and reusable.

ğŸ” Why This Matters

Continual Pretraining is risky: Updating even a well-aligned LLM can degrade its instruction-following ability by up to 10 points.

Instruction Residuals fix this: Our technique restores, and often improves, instruction behavior with zero extra tuning.

Architecture-agnostic: Works across model families like LLaMa and Qwen.

Industry Adoption: Instruction Residuals are already being used in the wild:

DeepMindâ€™s GAIA project within the GemmaVerse ecosystem

CEIA-UFGâ€™s Gemma-3-Gaia-PT-BR-4b-it multilingual model on HuggingFace

ğŸ“Š Highlights from Our Findings

Restores instruction-following: After 1B tokens of continual pretraining, Instruction Residuals restore performance nearly to original levels, sometimes even exceeding them.

Cross-model portability: Residuals from one model (e.g., LLaMa 3.1) can improve instruction behavior in earlier versions (e.g., LLaMa 3), showing backward compatibility.

Generalizes to domain-tuned models: Applied to domain-specific models like LLaMa-DocChat, instruction residuals boost instruction benchmarks by up to 6 pointsâ€”without harming domain QA ability.

ğŸ§® 2000Ã— Less Compute, Comparable Performance

A detailed compute-performance analysis revealed that traditional instruction tuning demands nearly 2000Ã— more FLOPs than our residual approach. With the residual method, we deliver competitive accuracy on benchmarks like MMLU, IFEval, and GSM8K, at a fraction of the computational cost.

ğŸš€ Whatâ€™s Next?

This is just the beginning. Ongoing work focuses on:

Extending instruction residuals to smaller models (â‰¤1.5B) without quality loss.

Generalizing across model architectures like Mistral, Mixtral, and future Qwen variants.

Exploring residual approximation techniques when both the base and instruction-tuned models are not available.

ğŸ™ Acknowledgments

Thanks to my collaborators at Samsung R&D Institute, India and to the broader research community for supporting this work. The momentum weâ€™ve gainedâ€”both in academia and industryâ€”has been incredible.

Stay tuned for the ICML workshop talk and follow-up demos!

This post will show up by default. To disable scheduling of future posts, edit `config.yml` and set `future: false`. 
